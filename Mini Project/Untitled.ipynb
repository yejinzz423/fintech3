{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b9ee5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ì£¼ìš”ë‰´ìŠ¤ ê²Œì‹œê¸€ ëª©ë¡ ìˆ˜ì§‘ ì‹œì‘ (í˜ì´ì§€ 1 ~ 18)\n",
      "-> ì´ 0ê°œì˜ ì£¼ìš”ë‰´ìŠ¤ ê²Œì‹œê¸€ ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ.\n",
      "\n",
      "2. ê°œë³„ ë‰´ìŠ¤ ì›ë¬¸ ë§í¬ ì¶”ì¶œ ë° ë³¸ë¬¸ ìˆ˜ì§‘ ì‹œì‘ (ì´ ë‰´ìŠ¤ ê²Œì‹œê¸€ ìˆ˜: 0 )\n",
      "\n",
      "==============================================================================\n",
      "ğŸ”¥ ìµœì¢… ìˆ˜ì§‘ ì™„ë£Œ! ì´ 0ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ë¨.\n",
      "==============================================================================\n",
      "\n",
      "[ìˆ˜ì§‘ëœ ë°ì´í„°í”„ë ˆì„ ë¯¸ë¦¬ë³´ê¸°]\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: []\n",
      "\n",
      "[ë°ì´í„°í”„ë ˆì„ ì •ë³´]\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 0 entries\n",
      "Empty DataFrame\n",
      "None\n",
      "\n",
      "ë°ì´í„°ê°€ 'fintech_data_raw.csv' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ì´ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„ì„ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n"
     ]
    }
   ],
   "source": [
    "# ==============================================================================\n",
    "# í•€í…Œí¬ ì§€ì›ì„¼í„° ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘ íŒŒì´í”„ë¼ì¸\n",
    "# (18 í˜ì´ì§€ ë°˜ë³µ -> ì£¼ìš”ë‰´ìŠ¤ ê²Œì‹œê¸€ ì§„ì… -> ê°œë³„ ì›ë¬¸ ë§í¬ ì¶”ì¶œ -> ë³¸ë¬¸ í…ìŠ¤íŠ¸ ìˆ˜ì§‘)\n",
    "# ==============================================================================\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import pandas as pd\n",
    "import re\n",
    "import time\n",
    "# from tqdm import tqdm # tqdm ë¼ì´ë¸ŒëŸ¬ë¦¬ ì‚¬ìš© ì œê±°\n",
    "\n",
    "# ê¸°ë³¸ URL ì„¤ì •\n",
    "LIST_URL = \"https://fintech.or.kr/web/board/boardContentsList.do\"  # ê²Œì‹œê¸€ ëª©ë¡ URL\n",
    "VIEW_URL = \"https://fintech.or.kr/web/board/boardContentsView.do\"  # ê²Œì‹œê¸€ ìƒì„¸ ë³´ê¸° URL\n",
    "\n",
    "# ìµœì¢… ìˆ˜ì§‘ ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "final_data = []\n",
    "\n",
    "# ==============================================================================\n",
    "# 1ë‹¨ê³„: ì „ì²´ 18 í˜ì´ì§€ ìˆœíšŒí•˜ë©° 'í•€í…Œí¬ ì£¼ìš”ë‰´ìŠ¤' ê²Œì‹œê¸€ ë§í¬ ìˆ˜ì§‘\n",
    "# ==============================================================================\n",
    "def scrape_major_news_list(start_page, end_page):\n",
    "    \"\"\"\n",
    "    1í˜ì´ì§€ë¶€í„° 18í˜ì´ì§€ê¹Œì§€ ìˆœíšŒí•˜ë©° 'í•€í…Œí¬ ì£¼ìš”ë‰´ìŠ¤' ê²Œì‹œê¸€ì˜ ì œëª©, ë‚ ì§œ, ë‚´ë¶€ ë§í¬(JavaScript)ë¥¼ ìˆ˜ì§‘í•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    print(f\"1. ì£¼ìš”ë‰´ìŠ¤ ê²Œì‹œê¸€ ëª©ë¡ ìˆ˜ì§‘ ì‹œì‘ (í˜ì´ì§€ {start_page} ~ {end_page})\")\n",
    "    \n",
    "    major_news_posts = []\n",
    "    \n",
    "    # 1. 18 í˜ì´ì§€ ë°˜ë³µë¬¸ ì‹œì‘\n",
    "    for page in range(start_page, end_page + 1):\n",
    "        try:\n",
    "            # í˜ì´ì§€ íŒŒë¼ë¯¸í„°\n",
    "            params = { \"board_id\": \"6\", \"menu_id\": \"6500\", \"miv_pageNo\": page }\n",
    "            r = requests.get(LIST_URL, params=params, timeout=5)\n",
    "            r.raise_for_status()\n",
    "            soup = bs(r.content, \"lxml\")\n",
    "            \n",
    "            # ê²Œì‹œê¸€ ì œëª© a íƒœê·¸ ì„ íƒ\n",
    "            page_list = soup.select(\".title a\")\n",
    "            \n",
    "            for a_tag in page_list:\n",
    "                title = a_tag.get_text(strip=True)\n",
    "                \n",
    "                # 'í•€í…Œí¬ ì£¼ìš”ë‰´ìŠ¤' ì œëª© í•„í„°ë§\n",
    "                if \"í•€í…Œí¬ ì£¼ìš”ë‰´ìŠ¤\" in title:\n",
    "                    link_js = a_tag.get(\"href\")  # ex: javascript:fn_view('contents_id', '6')\n",
    "                    \n",
    "                    # ë‚ ì§œ ì¶”ì¶œ: ë™ì¼ tr ë‚´ì˜ ë§ˆì§€ë§‰ td\n",
    "                    tr_tag = a_tag.find_parent(\"tr\")\n",
    "                    date = tr_tag.select(\"td\")[-1].get_text(strip=True) if tr_tag else \"\"\n",
    "                    \n",
    "                    # contents_id ì¶”ì¶œ (ê°œë³„ ë‰´ìŠ¤ ì›ë¬¸ ëª©ë¡ì„ ê°€ì ¸ì˜¤ê¸° ìœ„í•´ í•„ìš”)\n",
    "                    match = re.search(r\"fn_view\\('([a-f0-9]+)'\", link_js)\n",
    "                    contents_id = match.group(1) if match else None\n",
    "                    \n",
    "                    if contents_id:\n",
    "                        major_news_posts.append({\n",
    "                            'major_title': title,\n",
    "                            'major_date': date,\n",
    "                            'contents_id': contents_id\n",
    "                        })\n",
    "            \n",
    "            time.sleep(0.5) # ì„œë²„ ë¶€í•˜ ë°©ì§€ë¥¼ ìœ„í•œ ë”œë ˆì´\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Error fetching page {page}: {e}\")\n",
    "            continue\n",
    "            \n",
    "    print(f\"-> ì´ {len(major_news_posts)}ê°œì˜ ì£¼ìš”ë‰´ìŠ¤ ê²Œì‹œê¸€ ëª©ë¡ ìˆ˜ì§‘ ì™„ë£Œ.\")\n",
    "    return major_news_posts\n",
    "\n",
    "# ==============================================================================\n",
    "# 2ë‹¨ê³„: ì£¼ìš”ë‰´ìŠ¤ ê²Œì‹œê¸€ ì§„ì… ë° ê°œë³„ ë‰´ìŠ¤ ì›ë¬¸ ë§í¬ ì¶”ì¶œ\n",
    "# ==============================================================================\n",
    "def scrape_individual_news_links(contents_id, major_title, major_date):\n",
    "    \"\"\"\n",
    "    ì£¼ìš”ë‰´ìŠ¤ ê²Œì‹œê¸€(contents_id)ì— POST ìš”ì²­ì„ ë³´ë‚´ ê°œë³„ ë‰´ìŠ¤ ì›ë¬¸ ë§í¬ì™€ ì œëª©ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.\n",
    "    \"\"\"\n",
    "    post_payload = {\n",
    "        \"method\": \"searchCorpList\", \n",
    "        \"orderMode\": 3, \n",
    "        \"orderStat\": \"D\",\n",
    "        \"searchType\": 13, \n",
    "        \"fiscalYearEnd\": \"all\", \n",
    "        \"location\": \"all\", \n",
    "        \"contents_id\": contents_id,  # ë™ì ìœ¼ë¡œ ì„¤ì •\n",
    "        \"homepage\": \"F\", \n",
    "        \"up_menu_if\": 6000\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        r = requests.post(VIEW_URL, data=post_payload, timeout=10)\n",
    "        r.raise_for_status()\n",
    "        soup = bs(r.content, 'lxml')\n",
    "        \n",
    "        individual_news = []\n",
    "        # ì›ë¬¸ ë§í¬ëŠ” ê²Œì‹œê¸€ ë‚´ìš© ì˜ì—­ì˜ <a> íƒœê·¸ì— ìˆìŠµë‹ˆë‹¤.\n",
    "        fintech_list = soup.select(\"td a\")\n",
    "        \n",
    "        for a_tag in fintech_list:\n",
    "            news_title = a_tag.text.strip()\n",
    "            news_link = a_tag.get('href', '').strip()\n",
    "            \n",
    "            # ìœ íš¨í•œ HTTP ë§í¬ë§Œ ìˆ˜ì§‘í•˜ê³ , ì œëª©ì´ ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©°, ë¶ˆí•„ìš”í•œ Footer ë§í¬ ì œê±°\n",
    "            if news_link.startswith('http') and news_title and news_title != 'ì´ì „':\n",
    "                individual_news.append({\n",
    "                    'date': major_date,\n",
    "                    'major_title': major_title,\n",
    "                    'news_title': news_title,\n",
    "                    'news_link': news_link\n",
    "                })\n",
    "        \n",
    "        return individual_news\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error fetching contents_id {contents_id}: {e}\")\n",
    "        return []\n",
    "\n",
    "# ==============================================================================\n",
    "# 3ë‹¨ê³„: ì™¸ë¶€ ë‰´ìŠ¤ ì›ë¬¸ ë§í¬ì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (ì‚¬ìš©ì ì œê³µ í•¨ìˆ˜ ê¸°ë°˜)\n",
    "# ==============================================================================\n",
    "def get_article_text(url):\n",
    "    \"\"\"\n",
    "    ì™¸ë¶€ ë‰´ìŠ¤ ì›ë¬¸ ë§í¬ì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ (íƒ€ì„ì•„ì›ƒ ë° ì˜ˆì™¸ ì²˜ë¦¬ í¬í•¨)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # ëŒ€ë¶€ë¶„ì˜ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ëŠ” User-Agentê°€ ì—†ìœ¼ë©´ ì ‘ê·¼ì´ ë§‰í™ë‹ˆë‹¤.\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "        r = requests.get(url, timeout=10, headers=headers)\n",
    "        r.encoding = 'utf-8' # ì¸ì½”ë”© ì„¤ì •\n",
    "        r.raise_for_status()\n",
    "        soup = bs(r.text, 'lxml')\n",
    "\n",
    "        # ë„¤ì´ë²„, ë‹¤ìŒ ë“± ì¼ë°˜ì ì¸ ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œì„ ìœ„í•œ ì„ íƒì ì‹œë„\n",
    "        # dic_areaëŠ” ë„¤ì´ë²„ ë‰´ìŠ¤ ë³¸ë¬¸ ì„ íƒì\n",
    "        paragraphs = soup.select('#dic_area, .article_view, #articeBody, #articleBodyContents')\n",
    "        \n",
    "        if paragraphs:\n",
    "            # ì¶”ì¶œëœ ëª¨ë“  ë‹¨ë½ì„ ì¤„ë°”ê¿ˆìœ¼ë¡œ ì—°ê²°\n",
    "            content = \"\\n\".join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])\n",
    "            return content\n",
    "        else:\n",
    "            # ë³¸ë¬¸ ì„ íƒìë¡œ ì°¾ì§€ ëª»í–ˆì„ ê²½ìš°, ë¬¸ì„œ ì „ì²´ í…ìŠ¤íŠ¸ ë°˜í™˜ (ì¡ìŒ ë§ìŒ)\n",
    "            return soup.body.get_text(strip=True)[:1000] # ë„ˆë¬´ ê¸¸ì–´ì§€ëŠ” ê²ƒì„ ë°©ì§€\n",
    "\n",
    "    except Exception as e:\n",
    "        # print(f\"Error fetching {url}: {e}\") # ë””ë²„ê¹… ì‹œì—ë§Œ ì‚¬ìš©\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "# ==============================================================================\n",
    "# 4ë‹¨ê³„: ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰\n",
    "# ì´ ë¶€ë¶„ì´ ì£¼ìš”ë‰´ìŠ¤ ê²Œì‹œê¸€(contents_id)ì„ ë°˜ë³µí•˜ê³ , \n",
    "# ê·¸ ì•ˆì—ì„œ ê°œë³„ ë‰´ìŠ¤ ì›ë¬¸ ë§í¬(news_link)ë¥¼ ë°˜ë³µí•˜ì—¬ ë³¸ë¬¸ì„ ì¶”ì¶œí•˜ëŠ” í•µì‹¬ ë¡œì§ì…ë‹ˆë‹¤.\n",
    "# ==============================================================================\n",
    "\n",
    "# 1. ì£¼ìš” ë‰´ìŠ¤ ê²Œì‹œê¸€ ëª©ë¡ ìˆ˜ì§‘ (1í˜ì´ì§€ë¶€í„° 18í˜ì´ì§€ê¹Œì§€)\n",
    "major_posts = scrape_major_news_list(start_page=1, end_page=18)\n",
    "\n",
    "# 2. ê° ê²Œì‹œê¸€ì— ì§„ì…í•˜ì—¬ ê°œë³„ ë‰´ìŠ¤ ë§í¬ ìˆ˜ì§‘ ë° ë³¸ë¬¸ ì¶”ì¶œ\n",
    "print(\"\\n2. ê°œë³„ ë‰´ìŠ¤ ì›ë¬¸ ë§í¬ ì¶”ì¶œ ë° ë³¸ë¬¸ ìˆ˜ì§‘ ì‹œì‘ (ì´ ë‰´ìŠ¤ ê²Œì‹œê¸€ ìˆ˜:\", len(major_posts), \")\")\n",
    "\n",
    "# ì§„í–‰ ìƒí™©ì„ í‘œì‹œí•˜ê¸° ìœ„í•´ ìˆ˜ë™ ì¹´ìš´í„° ì‚¬ìš©\n",
    "for i, post in enumerate(major_posts): # <-- 2. 'í•€í…Œí¬ ì£¼ìš”ë‰´ìŠ¤' ê²Œì‹œê¸€ ë°˜ë³µ\n",
    "    \n",
    "    # í˜„ì¬ ì§„í–‰ ìƒí™© ì¶œë ¥\n",
    "    print(f\"[{i+1}/{len(major_posts)}] ê²Œì‹œê¸€ ìˆ˜ì§‘ ì¤‘: {post['major_title']} ({post['major_date']})\")\n",
    "    \n",
    "    # 2ë‹¨ê³„ ì‹¤í–‰: ê°œë³„ ë‰´ìŠ¤ ì›ë¬¸ ë§í¬ ì¶”ì¶œ\n",
    "    individual_news_list = scrape_individual_news_links(\n",
    "        contents_id=post['contents_id'], \n",
    "        major_title=post['major_title'], \n",
    "        major_date=post['major_date']\n",
    "    )\n",
    "    \n",
    "    # 3ë‹¨ê³„ ì‹¤í–‰: ê° ì›ë¬¸ ë§í¬ì—ì„œ ë³¸ë¬¸ í…ìŠ¤íŠ¸ ì¶”ì¶œ\n",
    "    for news_item in individual_news_list: # <-- 3. ê°œë³„ ë‰´ìŠ¤ ì›ë¬¸ ë§í¬ ë°˜ë³µ\n",
    "        article_body = get_article_text(news_item['news_link'])\n",
    "        \n",
    "        # ë³¸ë¬¸ì´ ì„±ê³µì ìœ¼ë¡œ ì¶”ì¶œëœ ê²½ìš°ì—ë§Œ ìµœì¢… ë°ì´í„°ì— ì¶”ê°€\n",
    "        if article_body:\n",
    "            final_data.append({\n",
    "                'date': news_item['date'],\n",
    "                'major_title': news_item['major_title'],\n",
    "                'title': news_item['news_title'],\n",
    "                'link': news_item['news_link'],\n",
    "                'body': article_body\n",
    "            })\n",
    "    \n",
    "    time.sleep(1) # ê° ì£¼ìš”ë‰´ìŠ¤ ê²Œì‹œê¸€ ì‚¬ì´ì˜ ë”œë ˆì´\n",
    "\n",
    "# ==============================================================================\n",
    "# 5ë‹¨ê³„: ê²°ê³¼ ì •ë¦¬ ë° ì €ì¥\n",
    "# ==============================================================================\n",
    "\n",
    "df_final = pd.DataFrame(final_data)\n",
    "\n",
    "print(\"\\n==============================================================================\")\n",
    "print(f\"ğŸ”¥ ìµœì¢… ìˆ˜ì§‘ ì™„ë£Œ! ì´ {len(df_final)}ê°œì˜ ë‰´ìŠ¤ ê¸°ì‚¬ ë³¸ë¬¸ ìˆ˜ì§‘ë¨.\")\n",
    "print(\"==============================================================================\")\n",
    "print(\"\\n[ìˆ˜ì§‘ëœ ë°ì´í„°í”„ë ˆì„ ë¯¸ë¦¬ë³´ê¸°]\")\n",
    "print(df_final.head())\n",
    "print(\"\\n[ë°ì´í„°í”„ë ˆì„ ì •ë³´]\")\n",
    "print(df_final.info())\n",
    "\n",
    "# ê²°ê³¼ë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥\n",
    "CSV_FILE = 'fintech_data_raw.csv'\n",
    "df_final.to_csv(CSV_FILE, index=False, encoding='utf-8-sig')\n",
    "print(f\"\\në°ì´í„°ê°€ '{CSV_FILE}' íŒŒì¼ë¡œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì œ ì´ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë¶„ì„ì„ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
